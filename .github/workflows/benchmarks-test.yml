#this workflow run benchmarks

name: Benchmarks

on:
  push:
    branches:
      - benchmark_test

  workflow_dispatch:
    inputs:
      os:
        description: 'choose OS'
        required: true
        default: 'Ubuntu'
        type: choice
        options:
          - ubuntu
          - windows
      save:
        description: 'save log'
        default: false
        required: false
        type: boolean

env:
  PYTHON_V: '3.10'

jobs:
  benchmark:
    name: Run pytest-benchmark benchmark example
    # ${{ github.event.inputs.os }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: ${{env.PYTHON_V}}

      - name: Install dependencies
        run: |
            python -m pip install --upgrade pip
            python -m pip install pytest pytest-benchmark
            python -m pip install .

      - name: Run benchmark
        run: |
          [ ! -d .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit ] && mkdir .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit
          echo "Tag name from github.ref_name: ${{ steps.get_version.outputs.VERSION }}"
          echo  ${{ steps.branch_name.outputs.SOURCE_TAG }}
          pytest benchmark/benchmark_stepper.py --benchmark-json .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit/new.json >> result
      - name: compare benchmark
        run: |
          ls -a .benchmarks/*/
          pytest-benchmark compare ${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit/* >> compare_result

      # upload the result on action git
      - name: Upload Artifact
        uses: actions/upload-artifact@v3
        with:
          name: my-artifact
          path: |
           compare_result 
           result 
           .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit/new.json
      - name: Download all workflow run artifacts
        uses: actions/download-artifact@v3
        with:
            name: my-artifact

      - if: ${{ github.event.inputs.save == true }}
      - name: Commit report
        run: |
          git config --global user.name 'test'
          git config --global user.email 'test@users.noreply.github.com'
          git add .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit/new.json
          git commit -am "Automated report"
          git push
      - name: store in page
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Python Benchmark with pytest-benchmark
          tool: 'pytest'
          output-file-path: .benchmarks/${{runner.os}}-CPython-${{env.PYTHON_V}}-64bit/new.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # alert-threshold: '200%'
          # comment-on-alert: true
          # fail-on-alert: true