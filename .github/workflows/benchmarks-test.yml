#this workflow run benchmarks

name: Benchmarks

on:
  push:
     branches:
       - benchmark_test

  workflow_dispatch:
    inputs:
      os:
        description: 'choose OS'
        required: true
        default: 'Ubuntu'
        type: choice
        options:
          - ubuntu
          - windows
      save:
        description: 'save log'
        default: false
        required: false
        type: boolean

env:
  PYTHON_V: '3.10'

jobs:
  benchmark:
    name: Performance regression check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: ${{env.PYTHON_V}}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install pytest pytest-benchmark pytest-benchmark[histogram]
          python -m pip install .
      - uses: actions/setup-go@v1
      # Run benchmark with `go test -bench` and stores the output to a file
      - name: Run benchmark
        run: pytest benchmark/benchmark_stepper.py | tee output.txt
      # Download previous benchmark result from cache (if exists)
      - name: Download previous benchmark data
        uses: actions/cache@v1
        with:
          path: ./cache
          key: ${{ runner.os }}-benchmark
      # Run `github-action-benchmark` action
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          # What benchmark tool the output.txt came from
          tool: 'go'
          # Where the output from the benchmark tool is stored
          output-file-path: output.txt
          # Where the previous data file is stored
          external-data-json-path: ./cache/benchmark-data.json
          # Workflow will fail when an alert happens
          fail-on-alert: true
      # Upload the updated cache file for the next job by actions/cache